<!DOCTYPE html>
<html><head lang="en"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Finger Snap Detection - Luca Manolache</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Implementation of a ConvNet model using PyTorch to detect finger snaps in audio recordings, including data collection, model architecture design, and training setup." />
	<meta property="og:image" content=""/>
	<meta property="og:url" content="http://localhost:1313/posts/2023-05-15-snap-detection/">
  <meta property="og:site_name" content="Luca Manolache">
  <meta property="og:title" content="Finger Snap Detection">
  <meta property="og:description" content="Implementation of a ConvNet model using PyTorch to detect finger snaps in audio recordings, including data collection, model architecture design, and training setup.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-05-15T19:44:45-07:00">
    <meta property="article:modified_time" content="2023-05-15T19:44:45-07:00">
    <meta property="article:tag" content="Ml">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Finger Snap Detection">
  <meta name="twitter:description" content="Implementation of a ConvNet model using PyTorch to detect finger snaps in audio recordings, including data collection, model architecture design, and training setup.">

        <link href="http://localhost:1313/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	<link rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/main.ebab3cdab514812ae87420376868866f1f961f23c3eba7d8d1f16eb62308a54b.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="http://localhost:1313/css/dark.0953caef1b838ade4b219e3b6b9b54f92357b9ff5c3f3ddec1c1f583b807cbbc.css" media="(prefers-color-scheme: dark)"  />
		
		<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/custom.c98772c6a0e44a03c25b6b1d0a53b4ee8e5f95502683a7993fd45decfcbdc6d0.css">
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="http://localhost:1313/">Luca Manolache</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/posts">Posts</a>
		
		<a href="/papers">Papers</a>
		
		<a href="/teaching">Teaching</a>
		
		<a href="/resume">Resume</a>
		
		<a href="/tags">Tags</a>
		
		
	</nav>
</header>

<main>
  <article>
    <div class="post-container">
      
      <div class="post-content">
        <div class="title">
          <h1 class="title">Finger Snap Detection</h1>
          <div class="meta">Posted on May 15, 2023</div>
        </div>
        
        <section class="body">
          <h2 id="libraries">Libraries</h2>
<p>To detect finger snapping, I will use my favorite machine learning library <code>pytorch</code>.
One of the main reasons I am using <code>pytorch</code> is because it has <code>pytorch-audio</code>, an amazing helper librarie to work with audio.
Another reason is for the helper library, <code>pytorch-lightning</code> which makes training models easier and has automatic integration with <code>tensorboard</code>.
To install all the libraries I am using, do <code>pip install torch torchaudio lightning torchvision tensorboard sounddevice</code>.</p>
<h2 id="dataset">Dataset</h2>
<p>When I started this project I was using <a href="https://research.google.com/audioset/">Google&rsquo;s audioset</a>.
However, this had far more data than I needed and was a pain to work with.
Additionally, I found better performance when using my own dataset which I recorded.</p>
<p>To make a dataset, I recommend coding a simple program to continously record 2 second intervals.
I would then snap my fingers or just do random things without snapping my fingers while the program made multiple files of recordings.
After doing this for a few minutes I would sort all my data manually into two files <code>snaps</code> and <code>no-snaps</code> to be used for training.</p>
<p>If you want to go by this approach, this code should be able to do this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> uuid
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sounddevice <span style="color:#66d9ef">as</span> sd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.io.wavfile <span style="color:#f92672">import</span> write
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    fs <span style="color:#f92672">=</span> <span style="color:#ae81ff">44100</span>  <span style="color:#75715e"># Sample rate</span>
</span></span><span style="display:flex;"><span>    seconds <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># Duration of each recording</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        recording <span style="color:#f92672">=</span> sd<span style="color:#f92672">.</span>rec(int(seconds <span style="color:#f92672">*</span> fs), samplerate<span style="color:#f92672">=</span>fs, channels<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        sd<span style="color:#f92672">.</span>wait()
</span></span><span style="display:flex;"><span>        write(<span style="color:#e6db74">&#39;data/</span><span style="color:#e6db74">{id}</span><span style="color:#e6db74">.wav&#39;</span><span style="color:#f92672">.</span>format(id<span style="color:#f92672">=</span>str(uuid<span style="color:#f92672">.</span>uuid1())), fs, recording)
</span></span></code></pre></div><p>To break down the code, <code>fs = 44100</code> sets the sample rate.
This is a represents 44.1 kHz which is a set recording rate for most streaming and consumer audio.
Different microphones might have different sample rates.
<code>seconds = 2</code> is the length of each file recorded.
When writing, we create each file name with its own uuid in order to not have duplicate names (I originally just called the files 1, 2, 3,&hellip; but when restarting the code to take a break it would overwrite old recordings).
<code>uuid1()</code> creates a uuid based on the time, other methods from uuid use different criteria to generate the uuid.</p>
<p>I used this to get a dataset of around 50 snaps and 50 random noises for the binary classification problem of detecting snaps or not.
One tip I have after doing this several times is to make sure you have snaps while speaking.
When I originally recorded my dataset all of my snaps had no background noise which caused major issues detecting snaps while talking.</p>
<h2 id="model-architecture">Model Architecture</h2>
<p><img src="/images/snap-detection/model.png" alt="conv net architecture image">{: style=&ldquo;float: right&rdquo; width=&ldquo;250&rdquo;}</p>
<p>The model I am using is a simple ConvNet as shown to the right.
My original ConvNet performed far worse than the current one.
All of the changes I made were based off of the paper <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf">A ConvNet for the 2020s</a>.
In this, they suggested increasing the kernel size (for me this was from 3 -&gt; 7).
This by itself increased my models performance by ~4%.
Additionally, they suggested using <code>GeLu</code> instead of <code>ReLu</code> as the activation function.
Similar to their discoveries, this did not increase the performance nearly as much as increasing the kernel size.
However, this paper suggested using <code>LinearNorm</code> instead of <code>BatchNorm</code>.
When doing this, my model&rsquo;s performance dropped significantly (around 40%).</p>
<p>I tried implementing the rest of the suggestions offered by the paper, including adding their modified ResNet blocks, however this lowered the performance instead (94% -&gt; 60% with modifications).
When trying to implement their model, the performance initially dopped to ~54% which is only slightly better than randomly deciding.
Through testing, I found that by changing their <code>LinearNorm</code> to <code>BatchNorm</code> the performance could be brought up by around 10-20%.
By changing the size of each block, I managed to get to a maximam accuracy of 70%.
This, however, is far below my simple ConvNet&rsquo;s performance.
I believe this is because the problem of binary classification is far too simple for a large model and it most likely is overfitting.</p>
<h3 id="implementation">Implementation</h3>
<p>To implement this model, I first define a helper module for each convolutional, norm, and activation layers.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ConvBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, inp, out, kernel<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(inp, out, kernel_size<span style="color:#f92672">=</span>kernel, stride<span style="color:#f92672">=</span>stride, padding<span style="color:#f92672">=</span>padding)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(out)
</span></span><span style="display:flex;"><span>        init<span style="color:#f92672">.</span>kaiming_normal_(self<span style="color:#f92672">.</span>conv1<span style="color:#f92672">.</span>weight, a<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>For the actual model, I am using 4 of these blocks of increasing size.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AudioClassifier</span>(pl<span style="color:#f92672">.</span>LightningModule):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>accuracy <span style="color:#f92672">=</span> torchmetrics<span style="color:#f92672">.</span>Accuracy(task<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binary&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Conv layers</span>
</span></span><span style="display:flex;"><span>        conv_layers <span style="color:#f92672">=</span> [ConvBlock(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>), ConvBlock(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">16</span>), ConvBlock(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">32</span>), ConvBlock(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>)]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>conv_layers)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Binary Classifier</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ap <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AdaptiveAvgPool2d(output_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lin <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sigmoid <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BCELoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ap(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lin(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sigmoid(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>Just as a note, I am using pytorch lightning&rsquo;s <code>pl.LightningModule</code> instead of the normal pytorch <code>nn.Module</code>.
If you don&rsquo;t want to use pytorch lightning, ignore the rest.</p>
<p>In order to make training this model easier, I am using pytorch lightning&rsquo;s built in logging to log the accuracy after every epoch.
To do this, you must overwrite the <code>on_validation_epoch_end</code> methods that get called after every validation epoch.</p>
<p>Additionally, for a lightning module to be trained using a pytorch lightning <code>Trainer</code>, you must overwite two more methods to dictate how every training/validation step works.
To do this, add the following functions in the <code>AudioClassifier</code> class.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">configure_optimizers</span>(self):
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(self<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> optimizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_step</span>(self, train_batch, batch_idx):
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> train_batch
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>    y_hat <span style="color:#f92672">=</span> self(x)
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterion(y_hat, y)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>accuracy(y_hat, y)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;train_loss&#39;</span>, loss)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;train_acc_step&#39;</span>, self<span style="color:#f92672">.</span>accuracy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">validation_step</span>(self, train_batch, batch_idx):
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> train_batch
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>    y_hat <span style="color:#f92672">=</span> self(x)
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterion(y_hat, y)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>accuracy(y_hat, y)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;val_loss&#39;</span>, loss)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;val_acc_step&#39;</span>, self<span style="color:#f92672">.</span>accuracy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">on_training_epoch_end</span>(self):
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;train_acc_epoch&#39;</span>, self<span style="color:#f92672">.</span>accuracy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">on_validation_epoch_end</span>(self):
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;val_acc_epoch&#39;</span>, self<span style="color:#f92672">.</span>accuracy)
</span></span></code></pre></div><h2 id="training">Training</h2>
<h3 id="loading-the-data">Loading the Data</h3>
<p>The following assumes you have your data as a <code>.wav</code> file and set up in a <code>data/</code> directory that is split further in two directories, <code>snap</code> and <code>no_snap</code>.
The approach I used to create the dataset, I created a dataframe with the location of every file along with its classification (1 for snap and 0 for no snap) which would the be used to load each audio clip on demand.
To get every file in the <code>snap</code> directory, you can use <code>snaps = [f for f in os.listdir(&quot;data/snap&quot;) if isfile(join(&quot;data/snap&quot;, f))]</code>.
Then to create the dataframe you can append all of these to a dataframe (there is probably a more efficient way to do this, but since it is only being run once, it doesn&rsquo;t matter).
This results in the following code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_file</span>(split<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>):
</span></span><span style="display:flex;"><span>    snaps <span style="color:#f92672">=</span> [f <span style="color:#66d9ef">for</span> f <span style="color:#f92672">in</span> os<span style="color:#f92672">.</span>listdir(<span style="color:#e6db74">&#34;data/snap&#34;</span>) <span style="color:#66d9ef">if</span> isfile(join(<span style="color:#e6db74">&#34;data/snap&#34;</span>, f))]
</span></span><span style="display:flex;"><span>    no_snaps <span style="color:#f92672">=</span> [f <span style="color:#66d9ef">for</span> f <span style="color:#f92672">in</span> os<span style="color:#f92672">.</span>listdir(<span style="color:#e6db74">&#34;data/no_snap&#34;</span>) <span style="color:#66d9ef">if</span> isfile(join(<span style="color:#e6db74">&#34;data/no_snap&#34;</span>, f))]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    snapping <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;label&#39;</span>, <span style="color:#e6db74">&#39;file&#39;</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> snaps:
</span></span><span style="display:flex;"><span>        snapping<span style="color:#f92672">.</span>loc[len(snapping)] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;data/snap/</span><span style="color:#e6db74">{name}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(name<span style="color:#f92672">=</span>s)]
</span></span><span style="display:flex;"><span>    not_snapping <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;label&#39;</span>, <span style="color:#e6db74">&#39;file&#39;</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> no_snaps:
</span></span><span style="display:flex;"><span>        not_snapping<span style="color:#f92672">.</span>loc[len(not_snapping)] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;data/no_snap/</span><span style="color:#e6db74">{name}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(name<span style="color:#f92672">=</span>s)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    snapping_train <span style="color:#f92672">=</span> snapping<span style="color:#f92672">.</span>iloc[:int(len(snapping) <span style="color:#f92672">*</span> split)]
</span></span><span style="display:flex;"><span>    snapping_validation <span style="color:#f92672">=</span> snapping<span style="color:#f92672">.</span>iloc[int(len(snapping) <span style="color:#f92672">*</span> split):]
</span></span><span style="display:flex;"><span>    not_snapping_train <span style="color:#f92672">=</span> not_snapping<span style="color:#f92672">.</span>iloc[:int(len(not_snapping) <span style="color:#f92672">*</span> split)]
</span></span><span style="display:flex;"><span>    not_snapping_validation <span style="color:#f92672">=</span> not_snapping<span style="color:#f92672">.</span>iloc[int(len(not_snapping) <span style="color:#f92672">*</span> split):]
</span></span><span style="display:flex;"><span>    df_train <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([snapping_train, not_snapping_train])<span style="color:#f92672">.</span>reset_index()
</span></span><span style="display:flex;"><span>    df_eval <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([snapping_validation, not_snapping_validation])<span style="color:#f92672">.</span>reset_index()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> df_train, df_eval
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>meta_data, meta_data_eval <span style="color:#f92672">=</span> process_file()
</span></span></code></pre></div><p>For training/validation split, I choose an 80% split, however, if you find yourself lacking data you can choose a more conservative split like 90%.</p>
<h3 id="data-augmentation">Data Augmentation</h3>
<p>In order to make up for the little data I had, I used the following data augementation techniques found <a href="">here</a>.
<code>time_shift</code> will move the audio data around so it doesn&rsquo;t all start/end the same.
<code>spectro_augment</code> will take a spectogram (more on that later) and put lines through it, masking parts of the data.
<code>spectro_gram</code> will generate said spectogram given raw audio.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AudioUtil</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">time_shift</span>(aud, shift_limit):
</span></span><span style="display:flex;"><span>        sig, sr <span style="color:#f92672">=</span> aud
</span></span><span style="display:flex;"><span>        _, sig_len <span style="color:#f92672">=</span> sig<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        shift_amt <span style="color:#f92672">=</span> int(random<span style="color:#f92672">.</span>random() <span style="color:#f92672">*</span> shift_limit <span style="color:#f92672">*</span> sig_len)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> sig<span style="color:#f92672">.</span>roll(shift_amt), sr
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">spectro_augment</span>(spec, max_mask_pct<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, n_freq_masks<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, n_time_masks<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        _, n_mels, n_steps <span style="color:#f92672">=</span> spec<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        mask_value <span style="color:#f92672">=</span> spec<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>        aug_spec <span style="color:#f92672">=</span> spec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        freq_mask_param <span style="color:#f92672">=</span> max_mask_pct <span style="color:#f92672">*</span> n_mels
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(n_freq_masks):
</span></span><span style="display:flex;"><span>            aug_spec <span style="color:#f92672">=</span> torchaudio<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>FrequencyMasking(freq_mask_param)(aug_spec, mask_value)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        time_mask_param <span style="color:#f92672">=</span> max_mask_pct <span style="color:#f92672">*</span> n_steps
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(n_time_masks):
</span></span><span style="display:flex;"><span>            aug_spec <span style="color:#f92672">=</span> torchaudio<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>TimeMasking(time_mask_param)(aug_spec, mask_value)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> aug_spec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">spectro_gram</span>(aud, n_mels<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_fft<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, hop_len<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        sig, sr <span style="color:#f92672">=</span> aud
</span></span><span style="display:flex;"><span>        top_db <span style="color:#f92672">=</span> <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        spec <span style="color:#f92672">=</span> torchaudio<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>MelSpectrogram(sr, n_fft<span style="color:#f92672">=</span>n_fft, hop_length<span style="color:#f92672">=</span>hop_len, n_mels<span style="color:#f92672">=</span>n_mels)(sig)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        spec <span style="color:#f92672">=</span> torchaudio<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>AmplitudeToDB(top_db<span style="color:#f92672">=</span>top_db)(spec)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> spec
</span></span></code></pre></div><p><img src="/images/snap-detection/spec2.png" alt="augmented spectogram">{: style=&ldquo;float: right&rdquo; width=&ldquo;250&rdquo;}</p>
<p>Ok, spectograms.
A spectogram is a visual representation of an audio file (can be used by a ConvNet).
From what I understand, they show how much of a certain frequency appears at every point in time in an audio file.
These can end up looking quite nice in my opinion.
An example of an augmented one can be seen on the right.</p>
<h3 id="dataloader">Dataloader</h3>
<p>To load the data at a certain index, we simply look at the dataframe of files and their classifications and load it using <code>torchaudio</code>.
After loading the data, we apply the shift augmentation, turn it into a spectrogram, and add lines masking data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SnapDataset</span>(Dataset):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, meta_data):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>meta_data <span style="color:#f92672">=</span> meta_data
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sr <span style="color:#f92672">=</span> <span style="color:#ae81ff">44100</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>duration <span style="color:#f92672">=</span> <span style="color:#ae81ff">2000</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>shift_pct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__len__</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>meta_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__getitem__</span>(self, idx):
</span></span><span style="display:flex;"><span>        path <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>meta_data[<span style="color:#e6db74">&#39;file&#39;</span>][idx]
</span></span><span style="display:flex;"><span>        label <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>meta_data[<span style="color:#e6db74">&#39;label&#39;</span>][idx]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        aud <span style="color:#f92672">=</span> torchaudio<span style="color:#f92672">.</span>load(path)
</span></span><span style="display:flex;"><span>        shift_aud <span style="color:#f92672">=</span> AudioUtil<span style="color:#f92672">.</span>time_shift(aud, self<span style="color:#f92672">.</span>shift_pct)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sgram <span style="color:#f92672">=</span> AudioUtil<span style="color:#f92672">.</span>spectro_gram(shift_aud, n_mels<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_fft<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, hop_len<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>        aug_sgram <span style="color:#f92672">=</span> AudioUtil<span style="color:#f92672">.</span>spectro_augment(sgram, max_mask_pct<span style="color:#f92672">=</span><span style="color:#ae81ff">0.075</span>, n_freq_masks<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, n_time_masks<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> aug_sgram, label
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_data <span style="color:#f92672">=</span> SnapDataset(meta_data)
</span></span><span style="display:flex;"><span>test_data <span style="color:#f92672">=</span> SnapDataset(meta_data_eval)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_dataloader <span style="color:#f92672">=</span> DataLoader(train_data, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>test_dataloader <span style="color:#f92672">=</span> DataLoader(test_data, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><h3 id="fitting">Fitting</h3>
<p>Now that we have our dataloaders, pytorch lightning makes training the model simple.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AudioClassifier()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>early_stop_callback <span style="color:#f92672">=</span> EarlyStopping(
</span></span><span style="display:flex;"><span>    monitor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;val_acc_epoch&#39;</span>,
</span></span><span style="display:flex;"><span>    min_delta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.00</span>,
</span></span><span style="display:flex;"><span>    patience<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
</span></span><span style="display:flex;"><span>    verbose<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>    mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;max&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> pl<span style="color:#f92672">.</span>Trainer(callbacks<span style="color:#f92672">=</span>[early_stop_callback])
</span></span><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>fit(model, train_dataloader, test_dataloader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_scripted <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>jit<span style="color:#f92672">.</span>script(model)
</span></span><span style="display:flex;"><span>model_scripted<span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#39;models/model.pt&#39;</span>)
</span></span></code></pre></div><p>To prevent overfitting, we stop the model if after 5 epochs, the validation accuracy has not improved.
Finally, we save our model using torch script.</p>
<h2 id="live-detection">Live Detection</h2>
<p>This section is still a work in progress.
I have a rudementary method set up that could use plenty of improvements.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> sd<span style="color:#f92672">.</span>rec(int(seconds <span style="color:#f92672">*</span> fs), samplerate<span style="color:#f92672">=</span>fs, channels<span style="color:#f92672">=</span>channels, blocking<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> data[:<span style="color:#ae81ff">88000</span>]
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> preprocess(data)
</span></span><span style="display:flex;"><span>    y_hat <span style="color:#f92672">=</span> model(x<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    conf <span style="color:#f92672">=</span> y_hat<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Not a snap&#34;</span> <span style="color:#66d9ef">if</span> conf <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.65</span> <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;Snap&#34;</span>, conf)
</span></span></code></pre></div><p>This will record an audio clip of 2 seconds and run the model we trained on it.
It will then record another, and so on.
This is not ideal as we don&rsquo;t get feedback as soon as we snap, we only see if a snap occured in the last 2 seconds.
Additionally, this won&rsquo;t tell you if you snap multiple times within 2 seconds or only once.
I plan on making improvements to this later, maybe detecting how many times a snap was heard in the past 2 seconds (prob not this though), or decreasing the recording time to the length of the average snap.
While both these have their own individual issues, it is definetly better than my current approach.</p>

        </section>
        <div class="post-tags">
          
          
          <nav class="nav tags">
            <ul class="tags">
              
              <li><a href="/tags/ml">ml</a></li>
              
            </ul>
          </nav>
          
          
        </div>
      </div>

      
      
    </div>

    </article>
</main>
<footer>
  <div style="display:flex"><a class="soc" href="https://github.com/lucamanolache" rel="me" title="GitHub"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#github" />
</svg></a><a class="border"></a></div>
  <div class="footer-info">
    2025  Â© Luca Manolache 
  </div>
</footer>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
 </div>
    </body>
</html> 